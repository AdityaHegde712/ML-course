{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem statement: The venture capitalist fund has hired you as a datascienteist to develop a model which will tell you which companies are worth investing in based on profit (meaning here the dependent variable is profit). They want to also understand which companies work better in new york or cali, how marketing spend affects profit, etc. Basically parameter relationships as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "END = \"\\n-------------------------------------\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to decide which variables to keep and which to throw away because garbage_in = garbage_out. \n",
    "There's 5 ways to make a model:\n",
    "* All in - when you know all are necessary based on pre-existing knowledge.\n",
    "* Backward elimination:\n",
    "    * Select significance level (say 0.05)\n",
    "    * Fit full model with all variables\n",
    "    * Consider predictor with highest P value. If P value > SL, go to step 4, else last step.\n",
    "    * Remove predictor with highest P value.\n",
    "    * Fit model.\n",
    "    * Repeat from 3 steps until highest P value lesser than significance level. \n",
    "    * Now, your model is ready.\n",
    "    This is fastest and ideal for now\n",
    "* Forward Selection\n",
    "    * Select significance level (0.05)\n",
    "    * Fit all possible simple regression models for dependent variable. \n",
    "    * Select the one with the lowest P value.\n",
    "    * Keep variable, fit all the others again but with this one included. \n",
    "    * Select the one with the lowest P value. If P < SL, Go to prev step. Else, last step.\n",
    "    * Now, model is ready.\n",
    "* Bidirectional Elimination aka Stepwise Regression\n",
    "    * Select SL to enter and SL to stay in the model (SLEnter = 0.05, SLStay = 0.05)\n",
    "    * Perform forward selection (New vars must have P < SLENTER to enter)\n",
    "    * Perform all steps of Backward elimination (Old vars must have P < SLStay to stay)\n",
    "    * Repeat prev 2 until no new can enter and no old can exit\n",
    "    * Model ready\n",
    "* All Possible models (i.e. Score Comparison)\n",
    "    * Select criterion of goodness (like akaike criterion)\n",
    "    * Construct all possible regression models (2^n - 1 for n attributes)\n",
    "    * Select one with best criterion\n",
    "    * Ready model\n",
    "    We hate this because 10 columns mean 1023 models. It's pain. Very bad approach. Very resource consuming. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual Code\n",
    "Note that unlike in simple linear regression, feature scaling is unnecessary because the coefficients in the equation will handle the weightage and therefore the scaling of those variables.\n",
    "Also, in case of multiple linear regression, there's no need to go through the assumptions. After implementation, if the model has poor accuracy, that tells us that Multi Linear Regrssion is not the way to go, after which we just try something else. Simple. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First steps is data preprocessing\n",
    "df = pd.read_csv('50_Startups.csv')\n",
    "x = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, -1].values\n",
    "\n",
    "# Checked for missing and encoding required data. State requires one hot encoding.\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [3])], remainder='passthrough') # [3] means column 3 is being encoded.\n",
    "x = np.array(ct.fit_transform(x))\n",
    "\n",
    "# Splitting into train test\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do we need to do something to avoid the dummy variable trap? No need to do anything, the classes in sklearn take care of it.\n",
    "# Do we have to work with features to select best ones like backward elimination? No, the class also takes care of that.\n",
    "\n",
    "# Training the model\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(x_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted profits vs Real profits\n",
      "[[103015.2  103282.38]\n",
      " [132582.28 144259.4 ]\n",
      " [132447.74 146121.95]\n",
      " [ 71976.1   77798.83]\n",
      " [178537.48 191050.39]\n",
      " [116161.24 105008.31]\n",
      " [ 67851.69  81229.06]\n",
      " [ 98791.73  97483.56]\n",
      " [113969.44 110352.25]\n",
      " [167921.07 166187.94]]\n"
     ]
    }
   ],
   "source": [
    "# Since there's 4 features, we can't just plot a simple graph, we'd need a 5 dim graph here.\n",
    "# Instead, we display 2 vectors: Vectors of real profit on the test set, and vectors of the predicted profits to compare if it's close. \n",
    "# After this, we'll do metrics.\n",
    "\n",
    "# Displaying the results\n",
    "y_pred = regressor.predict(x_test)\n",
    "np.set_printoptions(precision=2) # 2 decimal accuracy\n",
    "print(\"Predicted profits vs Real profits\")\n",
    "print(np.concatenate((y_pred.reshape(len(y_pred), 1), y_test.reshape(len(y_test), 1)), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, some are okay, some aren't great, and some are amazing. This means that the Multiple Linear Regression model works on this dataset for sure. The goal is to be efficient, so we will still try other models and then select the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[181566.92]\n"
     ]
    }
   ],
   "source": [
    "# To make a prediction on a single value, say where R&D = 16000, Admin = 130000, Marketing = 300000, state = california.\n",
    "print(regressor.predict([[1,0,0, 160000, 130000, 300000]]))\n",
    "# 1,0,0 are dummy vars for the 3 possible states, Cali, Florida, or NY. Always enter them first, because dummy variables are always set in the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8.66e+01 -8.73e+02  7.86e+02  7.73e-01  3.29e-02  3.66e-02]\n",
      "42467.5292485298\n"
     ]
    }
   ],
   "source": [
    "# Also, to get the final equation out of the regressor,\n",
    "print(regressor.coef_)\n",
    "print(regressor.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means:\n",
    "Profit = 86.6×Dummy State 1−873×Dummy State 2+786×Dummy State 3+0.773×R&D Spend+0.0329×Administration+0.0366×Marketing Spend+42467.53"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
